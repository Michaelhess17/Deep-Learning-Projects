{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "df = pd.read_csv('train_data.csv')\n",
    "df2 = pd.read_csv('emotion.data')\n",
    "df3 = pd.read_csv('WritingStories_recall.csv')\n",
    "df2.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "labels = {'fun': 'joy', 'happiness': 'joy', 'hate': 'anger', 'worry': 'fear', 'anger': 'anger', 'love': 'love',\n",
    "          'sadness': 'sadness', 'surprise': 'surprise'}\n",
    "df['sentiment'] = df['sentiment'].map(labels)\n",
    "df.columns = ['emotions', 'text']\n",
    "df = pd.concat([df, df2])\n",
    "df = df[~df['emotions'].isnull()]\n",
    "df.emotions.value_counts().plot.bar()\n",
    "plt.show()\n",
    "# %%\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "input_sentences = [clean_text(text) for text in df[\"text\"].values.tolist()]\n",
    "labels = df[\"emotions\"].values.tolist()\n",
    "\n",
    "word2id = dict()\n",
    "label2id = dict()\n",
    "\n",
    "max_words = 0  # maximum number of words in a sentence\n",
    "\n",
    "# Construction of word2id dict\n",
    "for sentence in input_sentences:\n",
    "\tfor word in sentence:\n",
    "\t\t# Add words to word2id dict if not exist\n",
    "\t\tif word not in word2id:\n",
    "\t\t\tword2id[word] = len(word2id)\n",
    "\t# If length of the sentence is greater than max_words, update max_words\n",
    "\tif len(sentence) > max_words:\n",
    "\t\tmax_words = len(sentence)\n",
    "\n",
    "# Construction of label2id and id2label dicts\n",
    "label2id = {l: i for i, l in enumerate(set(labels))}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "print(id2label)\n",
    "# %%\n",
    "import keras\n",
    "\n",
    "# Encode input words and labels\n",
    "X = [[word2id[word] for word in sentence] for sentence in input_sentences]\n",
    "Y = [label2id[label] for label in labels]\n",
    "\n",
    "# Apply Padding to X\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X = pad_sequences(X, max_words)\n",
    "\n",
    "# Convert Y to numpy array\n",
    "Y = keras.utils.to_categorical(Y, num_classes=len(label2id), dtype='float32')\n",
    "\n",
    "# Print shapes\n",
    "print(\"Shape of X: {}\".format(X.shape))\n",
    "print(\"Shape of Y: {}\".format(Y.shape))\n",
    "# %%\n",
    "embedding_dim = 128  # The dimension of word embeddings\n",
    "\n",
    "# Define input tensor\n",
    "sequence_input = keras.Input(shape=(max_words,), dtype='int32')\n",
    "\n",
    "# Word embedding layer\n",
    "embedded_inputs = keras.layers.Embedding(len(word2id) + 1,\n",
    "                                         embedding_dim,\n",
    "                                         input_length=max_words)(sequence_input)\n",
    "\n",
    "# Apply dropout to prevent overfitting\n",
    "embedded_inputs = keras.layers.Dropout(0.4)(embedded_inputs)\n",
    "\n",
    "# Apply Bidirectional LSTM over embedded inputs\n",
    "lstm_outs = keras.layers.wrappers.Bidirectional(\n",
    "\tkeras.layers.LSTM(embedding_dim, return_sequences=True)\n",
    ")(embedded_inputs)\n",
    "\n",
    "# Apply dropout to LSTM outputs to prevent overfitting\n",
    "lstm_outs = keras.layers.Dropout(0.4)(lstm_outs)\n",
    "\n",
    "# Attention Mechanism - Generate attention vectors\n",
    "input_dim = int(lstm_outs.shape[2])\n",
    "permuted_inputs = keras.layers.Permute((2, 1))(lstm_outs)\n",
    "attention_vector = keras.layers.TimeDistributed(keras.layers.Dense(1))(lstm_outs)\n",
    "attention_vector = keras.layers.Reshape((max_words,))(attention_vector)\n",
    "attention_vector = keras.layers.Activation('softmax', name='attention_vec')(attention_vector)\n",
    "attention_vector = keras.layers.Dropout(0.4)(attention_vector)\n",
    "attention_output = keras.layers.Dot(axes=1)([lstm_outs, attention_vector])\n",
    "\n",
    "# Last layer: fully connected with softmax activation\n",
    "fc = keras.layers.Dense(embedding_dim, activation='relu')(attention_output)\n",
    "output = keras.layers.Dense(len(label2id), activation='softmax')(fc)\n",
    "\n",
    "# Finally building model\n",
    "model = keras.Model(inputs=[sequence_input], outputs=output)\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer='adam')\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(X, Y, epochs=2, batch_size=64, validation_split=0.2, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_with_attentions = keras.Model(inputs=model.input,\n",
    "                                    outputs=[model.output,\n",
    "                                             model.get_layer('attention_vec').output])\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Select random samples to illustrate\n",
    "mask = (df['text'].str.len() > 150)\n",
    "sample_text = ''\n",
    "for _ in range(5):\n",
    "\tsample_text = sample_text + random.choice(df.loc[mask]['text'].values.tolist())\n",
    "print(sample_text)\n",
    "\n",
    "# Encode samples\n",
    "tokenized_sample = clean_text(sample_text)\n",
    "encoded_samples = [[word2id[word] for word in tokenized_sample]]\n",
    "\n",
    "# Padding\n",
    "encoded_samples = keras.preprocessing.sequence.pad_sequences(encoded_samples, maxlen=max_words)\n",
    "\n",
    "# Make predictions\n",
    "label_probs, attentions = model_with_attentions.predict(encoded_samples)\n",
    "label_probs = {id2label[_id]: prob for (label, _id), prob in zip(label2id.items(), label_probs[0])}\n",
    "\n",
    "# Get word attentions using attenion vector\n",
    "token_attention_dic = {}\n",
    "max_score = 0.0\n",
    "min_score = 0.0\n",
    "for token, attention_score in zip(tokenized_sample, attentions[0][-len(tokenized_sample):]):\n",
    "\ttoken_attention_dic[token] = math.sqrt(attention_score)\n",
    "\n",
    "# VISUALIZATION\n",
    "import matplotlib.pyplot as plt;\n",
    "\n",
    "plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "\n",
    "def rgb_to_hex(rgb):\n",
    "\treturn '#%02x%02x%02x' % rgb\n",
    "\n",
    "\n",
    "def attention2color(attention_score):\n",
    "\tr = 255 - int(attention_score * 255)\n",
    "\tcolor = rgb_to_hex((255, r, r))\n",
    "\treturn str(color)\n",
    "\n",
    "\n",
    "# Build HTML String to viualize attentions\n",
    "html_text = \"<hr><p style='font-size: large'><b>Text:  </b>\"\n",
    "for token, attention in token_attention_dic.items():\n",
    "\thtml_text += \"<span style='background-color:{};'>{} <span> \".format(attention2color(attention),\n",
    "\t                                                                    token)\n",
    "html_text += \"</p>\"\n",
    "# Display text enriched with attention scores\n",
    "display(HTML(html_text))\n",
    "\n",
    "# PLOT EMOTION SCORES\n",
    "emotions = [label for label, _ in label_probs.items()]\n",
    "scores = [score for _, score in label_probs.items()]\n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.bar(np.arange(len(emotions)), scores, align='center', alpha=0.5,\n",
    "        color=['black', 'red', 'green', 'blue', 'cyan', \"purple\"])\n",
    "plt.xticks(np.arange(len(emotions)), emotions)\n",
    "plt.ylabel('Scores')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "pycharm-b2658bdf",
   "language": "python",
   "display_name": "PyCharm (Hopfield Networks)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}